{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0809698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import datetime, os\n",
    "#hide tf logs \n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # or any {'0', '1', '2'} \n",
    "#0 (default) shows all, 1 to filter out INFO logs, 2 to additionally filter out WARNING logs, and 3 to additionally filter out ERROR logs\n",
    "import scipy.optimize\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import seaborn as sns \n",
    "import codecs, json\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error \n",
    "\n",
    "# generates same random numbers each time\n",
    "np.random.seed(1234)\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3434ef03",
   "metadata": {},
   "source": [
    "# *Data Prep*\n",
    "\n",
    "Training and Testing data is prepared from the solution file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10140d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Import\n",
    "K_real = np.load('./K_tx_real_d_one.npy') # Values of K(t,x)\n",
    "Q_real = np.load('./Q_15minutes.npy') # Input Matrix of Model\n",
    "f_real = Q_real[:,1] # Inflow Rate\n",
    "v_real = Q_real[:,3] # Average Speed\n",
    "pi_real = np.load('./pitilde_15minutes.npy') # Values of pi(t,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d73809c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_real.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf51b6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_array = np.array(K_real[:,0:len(K_real[0])])\n",
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "plt.rcParams['font.size'] = '25'\n",
    "for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    label.set_fontsize(30)\n",
    "plt.imshow(K_array.T, cmap = 'Blues', interpolation='nearest')\n",
    "plt.colorbar(fraction=0.046, pad=0.04)\n",
    "plt.axis([0,2016,0,len(K_real[0])])\n",
    "plt.clim(0, 2500) \n",
    "ax.set_aspect(1.0/ax.get_data_ratio(), adjustable='box')\n",
    "plt.title('K(t,x) of Bathtub Model', fontsize=45)\n",
    "plt.xlabel('time step (1 time step = 15 minutes)', fontsize=35)\n",
    "plt.ylabel('Remaining Trip Distance (miles)', fontsize=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2057ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Scale\n",
    "N_timestep = 24*21*4 # 24 hours * 21 days * 15 minutes = total timestep\n",
    "N_diststep = 75 # 1 distance step = 1 mile\n",
    "Size_diststep = 1 # unit: miles\n",
    "N_trainingdays = 0 # Set zero (0)\n",
    "N_training = 24*4*N_trainingdays # Training data is all\n",
    "\n",
    "\n",
    "## Normalization [0,1]\n",
    "x = np.arange(0,int(N_diststep/Size_diststep))[:,None]   # x: step of remaining distance\n",
    "t = np.arange(0,N_timestep)[:,None]    # Time step \n",
    "usol = K_real.transpose() [:,N_training:]     # real value of K(t,x) ==> Target Variable\n",
    "usol_max = usol.max()\n",
    "\n",
    "## Normalization [0,1]\n",
    "x = (x - x.min()) / (x.max() - x.min())\n",
    "t = (t - t.min()) / (t.max() - t.min())\n",
    "f_real = (f_real - usol.min()) / (usol.max() - usol.min())  # usol과 같은 dimension이어야 함. 단위 같기 때문.\n",
    "v_real = (v_real - v_real.min()) / (v_real.max() - v_real.min())\n",
    "usol = (usol - usol.min()) / (usol.max()-usol.min())\n",
    "\n",
    "X, T = np.meshgrid(x,t)                     # makes 2 arrays X and T such that u(X[i],T[j])=usol[i][j] are a tuple\n",
    "\n",
    "\n",
    "multiplier_dist = int(N_diststep/Size_diststep)-1\n",
    "multiplier_time = N_timestep-1\n",
    "multiplier_u = usol_max\n",
    "print('multiplier distance = ', multiplier_dist)\n",
    "print('multiplier time = ', multiplier_time)\n",
    "print('usol max = ', usol_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d21f3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Size_diststep = 25 # unit: miles\n",
    "size_input = int(N_diststep/Size_diststep)*N_timestep # Total number of grid points\n",
    "# size_input = int(N_diststep/Size_diststep)*2 + N_timestep*7 # Boundary\n",
    "N_u = int(size_input*0.2) # Total number of data points for 'u' ==> Training data from grid points\n",
    "N_f = N_u*2 #Total number of collocation (auxiliary) points \n",
    "alpha = 0.5 # weights of loss\n",
    "\n",
    "print('size input = ', size_input)\n",
    "print('N_u = ', N_u)\n",
    "print('N_f = ', N_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a801b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Previous version (before Aug 30)\n",
    "# size_input = int(N_diststep/Size_diststep)*N_timestep # All domain\n",
    "# # size_input = int(N_diststep/Size_diststep)*2 + N_timestep*7 # Boundary\n",
    "# N_u = int(size_input*0.5) #Total number of data points for 'u'\n",
    "# N_f = N_u*2 #Total number of collocation points \n",
    "# alpha = 0.5\n",
    "\n",
    "# print('size input = ', size_input)\n",
    "# print('N_u = ', N_u)\n",
    "# print('N_f = ', N_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc38df9a",
   "metadata": {},
   "source": [
    "# *Grid Data*\n",
    "\n",
    "We prepare the grid data to compare against the solution produced by the PINN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f4933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' X_u_test = [X[i],T[i]] [25600,2] for interpolation'''\n",
    "X_u_grid = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "\n",
    "# Domain bounds\n",
    "lb = X_u_grid[0]  # [-1. 0.]\n",
    "ub = X_u_grid[-1] # [1.  0.99]\n",
    "\n",
    "'''\n",
    "   Fortran Style ('F') flatten,stacked column wise!\n",
    "   u = [c1 \n",
    "        c2\n",
    "        .\n",
    "        .\n",
    "        cn]\n",
    "\n",
    "   u =  [25600x1] \n",
    "'''\n",
    "u_grid = usol.flatten('F')[:,None] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7f3c29",
   "metadata": {},
   "source": [
    "# *Training Data* (All domain)\n",
    "\n",
    "\n",
    "The boundary conditions serve as the test data for the PINN and the collocation points are generated using **Latin Hypercube Sampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99ace6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_u,N_f):\n",
    "\n",
    "#     '''Boundary Conditions'''\n",
    "\n",
    "#     #Initial Condition 0 =< x =< 9 (135 miles) and t = 0  \n",
    "#     leftedge_x = np.hstack((X[0,:][:,None], T[0,:][:,None])) #L1\n",
    "#     leftedge_u = usol[:,0][:,None]\n",
    "    \n",
    "#     #Initial Condition 0 =< x =< 9 (135 miles) and t = 2015  \n",
    "#     rightedge_x = np.hstack((X[0,:][:,None], T[-1,:][:,None])) #L1\n",
    "#     rightedge_u = usol[:,-1][:,None]\n",
    "\n",
    "#     #Boundary Condition x = 9 (135 miles) and 0 =< t =<2015\n",
    "#     topedge_x = np.hstack((X[:,-1][:,None], T[:,0][:,None])) #L3\n",
    "#     topedge_u = usol[-1,:][:,None]\n",
    "        \n",
    "#     #Boundary Condition x = 0 and 0 =< t =<2015\n",
    "#     bottomedge_x = np.hstack((X[:,0][:,None], T[:,0][:,None])) #L2\n",
    "#     bottomedge_u = usol[0,:][:,None]\n",
    "    \n",
    "#     #Boundary Condition x = 5 (25 miles) and 0 =< t =<2015\n",
    "#     midedge_x = np.hstack((X[:,5][:,None], T[:,0][:,None])) #L3\n",
    "#     midedge_u = usol[-1,:][:,None]\n",
    "    \n",
    "#     #Boundary Condition x = 10 (50 miles) and 0 =< t =<2015\n",
    "#     midedge1_x = np.hstack((X[:,10][:,None], T[:,0][:,None])) #L3\n",
    "#     midedge1_u = usol[-1,:][:,None]\n",
    "    \n",
    "#     #Boundary Condition x = 15 (75 miles) and 0 =< t =<2015\n",
    "#     midedge2_x = np.hstack((X[:,15][:,None], T[:,0][:,None])) #L3\n",
    "#     midedge2_u = usol[-1,:][:,None]\n",
    "    \n",
    "#     #Boundary Condition x = 20 (100 miles) and 0 =< t =<2015\n",
    "#     midedge3_x = np.hstack((X[:,20][:,None], T[:,0][:,None])) #L3\n",
    "#     midedge3_u = usol[-1,:][:,None]\n",
    "    \n",
    "#     #Boundary Condition x = 25 (125 miles) and 0 =< t =<2015\n",
    "#     midedge4_x = np.hstack((X[:,25][:,None], T[:,0][:,None])) #L3\n",
    "#     midedge4_u = usol[-1,:][:,None]\n",
    "    \n",
    "#     #Boundary Condition x = 2 and 0 =< t =<2015\n",
    "#     bottomedge_x_2 = np.hstack((X[:,1][:,None], T[:,0][:,None])) #L2\n",
    "#     bottomedge_u_2 = usol[2,:][:,None]\n",
    "    \n",
    "#     #Boundary Condition x = 1 and 0 =< t =<2015\n",
    "#     bottomedge_x_1 = np.hstack((X[:,1][:,None], T[:,0][:,None])) #L2\n",
    "#     bottomedge_u_1 = usol[1,:][:,None]\n",
    "\n",
    "\n",
    "#     all_X_u_train = np.vstack([leftedge_x, rightedge_x, bottomedge_x, topedge_x]) # X_u_train [4042,2] (456 = 10(L1)+2016(L2)+2016(L3))\n",
    "#     all_u_train = np.vstack([leftedge_u, rightedge_u, bottomedge_u, topedge_u])   #corresponding u [4042,]\n",
    "\n",
    "#     all_X_u_train = np.vstack([leftedge_x, rightedge_x, bottomedge_x, topedge_x, midedge_x, midedge1_x, midedge2_x, midedge3_x, midedge4_x]) # X_u_train [4042,2] (456 = 10(L1)+2016(L2)+2016(L3))\n",
    "#     all_u_train = np.vstack([leftedge_u, rightedge_u, bottomedge_u, topedge_u, midedge_u, midedge1_u, midedge2_u, midedge3_u, midedge4_u])   #corresponding u [4042,]\n",
    "\n",
    "#     all_X_u_train = np.vstack([leftedge_x, rightedge_x, bottomedge_x, topedge_x, midedge_x]) # X_u_train [4042,2] (456 = 10(L1)+2016(L2)+2016(L3))\n",
    "#     all_u_train = np.vstack([leftedge_u, rightedge_u, bottomedge_u, topedge_u, midedge_u])   #corresponding u [4042,]\n",
    "\n",
    "#     all_X_u_train = np.vstack([bottomedge_x, bottomedge_x_1, bottomedge_x_2]) # X_u_train [4042,2] (456 = 10(L1)+2016(L2)+2016(L3))\n",
    "#     all_u_train = np.vstack([bottomedge_u, bottomedge_u_1, bottomedge_u_2])   #corresponding u [4042,]\n",
    "    \n",
    "#     all_X_u_train = np.vstack([bottomedge_x, topedge_x]) # X_u_train [4042,2] (456 = 10(L1)+2016(L2)+2016(L3))\n",
    "#     all_u_train = np.vstack([bottomedge_u, topedge_u])   #corresponding u [4042,]\n",
    "\n",
    "    all_X_u_train = np.hstack((X.T[range(0,N_diststep,Size_diststep)].flatten()[:,None],T.T[range(0,N_diststep,Size_diststep)].flatten()[:,None]))\n",
    "    all_u_train = usol[range(0,N_diststep,Size_diststep)].flatten()[:,None]\n",
    "    \n",
    "    #choose random N_u points for training\n",
    "    np.random.seed(1234)\n",
    "    train_idx = np.random.choice(all_X_u_train.shape[0], N_u, replace=False) # ID of training data\n",
    "    test_idx = np.delete(np.arange(size_input), train_idx) # ID of testing data\n",
    "    \n",
    "    # Training Data\n",
    "    X_u_train = all_X_u_train[train_idx, :] #choose indices from  set 'idx' (x,t)\n",
    "    u_train = all_u_train[train_idx,:]      #choose corresponding u\n",
    "    \n",
    "    # Testing Data\n",
    "    X_u_test = all_X_u_train[test_idx, :]\n",
    "    u_test = all_u_train[test_idx,:]\n",
    "\n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "#     X_f_train = np.rint(lb + (ub-lb)*lhs(2,N_f))\n",
    "    X_f_train = (lb + (ub-lb)*lhs(2,N_f))\n",
    "#     X_f_train = np.unique(X_f_train, axis=0).astype(int) # 중복된 것 없앰\n",
    "    X_f_train = np.vstack((X_f_train, X_u_train)) # append training points to collocation points \n",
    "    \n",
    "    f_t = f_real[np.int0(X_f_train*multiplier_time)[:,1]][:,None]\n",
    "    pi_tx = pi_real[np.int0(X_f_train*multiplier_dist)[:,0],np.int0(X_f_train*multiplier_time)[:,1]][:,None]\n",
    "    v_t = v_real[np.int0(X_f_train*multiplier_time)[:,1]][:,None]\n",
    "\n",
    "    return X_f_train, X_u_train, u_train, X_u_test, u_test, f_t, pi_tx, v_t, all_X_u_train, all_u_train, train_idx, test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5deb76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_f_train, X_u_train, u_train, X_u_test, u_test, f_t, pi_tx, v_t, all_X_u_train, all_u_train, train_idx, test_idx  = trainingdata(N_u,N_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cee7395",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_u_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd341903",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_u_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6d2613",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_u_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bce240b",
   "metadata": {},
   "source": [
    "# **PINN**\n",
    "\n",
    "Generate a **PINN** of L hidden layers, each with n neurons. \n",
    "\n",
    "Initialization: ***Xavier***\n",
    "\n",
    "Activation: *tanh (x)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bbc528",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(tf.Module): \n",
    "    def __init__(self, layers, name=None):\n",
    "       \n",
    "        self.W = []  #Weights and biases\n",
    "        self.parameters = 0 #total number of parameters\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            \n",
    "            input_dim = layers[i]\n",
    "            output_dim = layers[i+1]\n",
    "            \n",
    "            #Xavier standard deviation \n",
    "            std_dv = np.sqrt((2.0/(input_dim + output_dim)))\n",
    "\n",
    "            #weights = normal distribution * Xavier standard deviation + 0\n",
    "            w = tf.random.normal([input_dim, output_dim], dtype = 'float64') * std_dv\n",
    "                       \n",
    "            w = tf.Variable(w, trainable=True, name = 'w' + str(i+1))\n",
    "\n",
    "            b = tf.Variable(tf.cast(tf.zeros([output_dim]), dtype = 'float64'), trainable = True, name = 'b' + str(i+1))\n",
    "                    \n",
    "            self.W.append(w)\n",
    "            self.W.append(b)\n",
    "            \n",
    "            self.parameters +=  input_dim * output_dim + output_dim\n",
    "    \n",
    "    def evaluate(self,x):\n",
    "        \n",
    "        x = (x-lb)/(ub-lb)\n",
    "        \n",
    "        a = x\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            \n",
    "            z = tf.add(tf.matmul(a, self.W[2*i]), self.W[2*i+1])\n",
    "            a = tf.nn.tanh(z)\n",
    "            \n",
    "        a = tf.add(tf.matmul(a, self.W[-2]), self.W[-1]) # For regression, no activation to last layer\n",
    "        return a\n",
    "    \n",
    "#     def speed(self,x):\n",
    "        \n",
    "#         a = tf.math.minimum(u_free, tf.abs(a_speed/x + b_speed))\n",
    "\n",
    "#         print('speed = ', a)\n",
    "#         print('k = ', x)\n",
    "\n",
    "#         return a\n",
    "    \n",
    "    def get_weights(self):\n",
    "\n",
    "        parameters_1d = []  # [.... W_i,b_i.....  ] 1d array\n",
    "        \n",
    "        for i in range (len(layers)-1):\n",
    "            \n",
    "            w_1d = tf.reshape(self.W[2*i],[-1])   #flatten weights \n",
    "            b_1d = tf.reshape(self.W[2*i+1],[-1]) #flatten biases\n",
    "            \n",
    "            parameters_1d = tf.concat([parameters_1d, w_1d], 0) #concat weights \n",
    "            parameters_1d = tf.concat([parameters_1d, b_1d], 0) #concat biases\n",
    "        \n",
    "        return parameters_1d\n",
    "        \n",
    "    def set_weights(self,parameters):\n",
    "                \n",
    "        for i in range (len(layers)-1):\n",
    "\n",
    "            shape_w = tf.shape(self.W[2*i]).numpy() # shape of the weight tensor\n",
    "            size_w = tf.size(self.W[2*i]).numpy() #size of the weight tensor \n",
    "            \n",
    "            shape_b = tf.shape(self.W[2*i+1]).numpy() # shape of the bias tensor\n",
    "            size_b = tf.size(self.W[2*i+1]).numpy() #size of the bias tensor \n",
    "                        \n",
    "            pick_w = parameters[0:size_w] #pick the weights \n",
    "            self.W[2*i].assign(tf.reshape(pick_w,shape_w)) # assign  \n",
    "            parameters = np.delete(parameters,np.arange(size_w),0) #delete \n",
    "            \n",
    "            pick_b = parameters[0:size_b] #pick the biases \n",
    "            self.W[2*i+1].assign(tf.reshape(pick_b,shape_b)) # assign \n",
    "            parameters = np.delete(parameters,np.arange(size_b),0) #delete \n",
    "\n",
    "            \n",
    "    def loss_BC(self,x,y):\n",
    "\n",
    "        loss_u = tf.reduce_mean(tf.square(y-self.evaluate(x)))\n",
    "        return loss_u\n",
    "    \n",
    "    def loss(self,x,y):\n",
    "\n",
    "        loss_u = self.loss_BC(x,y)\n",
    "\n",
    "        loss = loss_u\n",
    "\n",
    "        return loss, loss_u\n",
    "    \n",
    "    def optimizerfunc(self,parameters):\n",
    "        \n",
    "        self.set_weights(parameters)\n",
    "       \n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(self.trainable_variables)\n",
    "            \n",
    "            loss_val, loss_u= self.loss(X_u_train, u_train)\n",
    "            \n",
    "        grads = tape.gradient(loss_val,self.trainable_variables)\n",
    "                \n",
    "        del tape\n",
    "        \n",
    "        grads_1d = [ ] #flatten grads \n",
    "        \n",
    "        for i in range (len(layers)-1):\n",
    "\n",
    "            grads_w_1d = tf.reshape(grads[2*i],[-1]) #flatten weights \n",
    "            grads_b_1d = tf.reshape(grads[2*i+1],[-1]) #flatten biases\n",
    "\n",
    "            grads_1d = tf.concat([grads_1d, grads_w_1d], 0) #concat grad_weights \n",
    "            grads_1d = tf.concat([grads_1d, grads_b_1d], 0) #concat grad_biases\n",
    "\n",
    "        return loss_val.numpy(), grads_1d.numpy()\n",
    "    \n",
    "    def optimizer_callback(self,parameters):\n",
    "               \n",
    "        loss_value, loss_u = self.loss(X_u_train, u_train)\n",
    "        \n",
    "        u_pred = self.evaluate(X_u_test)\n",
    "        error_vec = np.linalg.norm((u_test-u_pred),2)/np.linalg.norm(u_test,2)\n",
    "        loss_record.append(loss_value)\n",
    "        l2_record.append(error_vec)\n",
    "        \n",
    "        tf.print(loss_value, loss_u, error_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a4f112",
   "metadata": {},
   "source": [
    "# *Model Training and Testing*\n",
    "\n",
    "A function '**model**' is defined to generate a NN as per the input set of hyperparameters, which is then trained and tested. The L2 Norm of the solution error is returned as a comparison metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca7adf5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training data\n",
    "X_f_train, X_u_train, u_train, X_u_test, u_test, f_t, pi_tx, v_t, all_X_u_train, all_u_train, train_idx, test_idx  = trainingdata(N_u,N_f)\n",
    "#alpha = int(N_f/N_u) #weight of loss function\n",
    "\n",
    "layers = np.array([2,20,20,20,20,20,20,20,20,1]) #8 hidden layers\n",
    "loss_record = []\n",
    "l2_record = []\n",
    "\n",
    "PINN = Sequentialmodel(layers)\n",
    "\n",
    "init_params = PINN.get_weights().numpy()\n",
    "\n",
    "start_time = time.time() \n",
    "\n",
    "# train the model with Scipy L-BFGS optimizer\n",
    "results = scipy.optimize.minimize(fun = PINN.optimizerfunc, \n",
    "                                  x0 = init_params, \n",
    "                                  args=(), \n",
    "                                  method='L-BFGS-B', \n",
    "                                  jac= True,        # If jac is True, fun is assumed to return the gradient along with the objective function\n",
    "                                  callback = PINN.optimizer_callback, \n",
    "                                  options = {'disp': None,\n",
    "                                            'maxcor': 100, \n",
    "                                            'ftol': 1 * np.finfo(float).eps,  #The iteration stops when (f^k - f^{k+1})/max{|f^k|,|f^{k+1}|,1} <= ftol\n",
    "#                                             'ftol': 1e-10,  #The iteration stops when (f^k - f^{k+1})/max{|f^k|,|f^{k+1}|,1} <= ftol\n",
    "                                            'gtol': 1e-10, \n",
    "                                            'maxfun':  50000, \n",
    "                                            'maxiter': 15000,\n",
    "                                            'iprint': -1,   #print update every 50 iterations\n",
    "                                            'maxls': 50})\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f9d8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed = end_time - start_time                \n",
    "print('Training time: %.2f' % (elapsed))\n",
    "\n",
    "print(results)\n",
    "\n",
    "PINN.set_weights(results.x)\n",
    "\n",
    "''' Model Accuracy ''' \n",
    "u_pred = PINN.evaluate(X_u_test)\n",
    "\n",
    "error_vec = np.linalg.norm((u_test-u_pred),2)/np.linalg.norm(u_test,2)        # Relative L2 Norm of the error (Vector)\n",
    "print('Test Error: %.5f'  % (error_vec))\n",
    "print('--------------------------------------------------')\n",
    "\n",
    "u_pred = np.reshape(u_pred,u_test.shape,order='F')                        # Fortran Style ,stacked column wise!\n",
    "\n",
    "# ''' Solution Plot '''\n",
    "# solutionplot(u_pred,X_u_train,u_train)\n",
    "y_test = u_test\n",
    "y_pred = u_pred\n",
    "\n",
    "print('MAE = ', mean_absolute_error(y_test, y_pred))\n",
    "print('MSE = ', mean_squared_error(y_test, y_pred) )\n",
    "print('RMSE = ', np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "print('L2 relative error =', error_vec)\n",
    "print('--------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde67a4c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "ax1 = plt.gca() \n",
    "ax1.set_facecolor('white')\n",
    "ax2 = ax1.twinx()\n",
    "line1 = ax1.plot(loss_record, linewidth = '3', color = \"royalblue\", label='Loss (left)')\n",
    "line2 = ax2.plot(l2_record, linewidth = '3', color = \"orange\", label='Relative L2 error (right)')\n",
    "lines = line1 + line2\n",
    "labels = [l.get_label() for l in lines]\n",
    "ax1.legend(lines, labels, loc='upper right')\n",
    "plt.title(\"Performance Metrics of Proposed PIML Model\", fontsize=30, fontweight=\"bold\")\n",
    "ax1.set_xlabel(\"Iterations\", fontsize=24, fontname = 'Arial', fontweight=\"bold\")\n",
    "ax1.set_ylabel(\"Loss\", fontsize=24, fontname = 'Arial', fontweight=\"bold\")\n",
    "ax2.set_ylabel(\"Relative L2 error\", fontsize=24, fontname = 'Arial', fontweight=\"bold\")\n",
    "fig.set_size_inches(16, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a40501",
   "metadata": {},
   "source": [
    "# 전체 그리드 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0720d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred = PINN.evaluate(X_u_grid)\n",
    "u_pred = np.reshape(u_pred,usol.shape,order='F')\n",
    "\n",
    "y_test = usol\n",
    "y_pred = u_pred\n",
    "\n",
    "error_vec = np.linalg.norm((y_test-y_pred),2)/np.linalg.norm(y_test,2)\n",
    "\n",
    "print('MAE = ', mean_absolute_error(y_test, y_pred))\n",
    "print('MSE = ', mean_squared_error(y_test, y_pred) )\n",
    "print('RMSE = ', np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "print('L2 relative error =', error_vec)\n",
    "print('--------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c2fe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_array = np.array(u_pred)\n",
    "K_array = (usol - K_array)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "plt.rcParams['font.size'] = '25'\n",
    "for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    label.set_fontsize(30)\n",
    "plt.imshow(K_array.transpose().T, cmap = 'bwr')\n",
    "plt.colorbar(fraction=0.046, pad=0.04)\n",
    "plt.axis([0,2016,0,75])\n",
    "plt.clim(-1, 1) \n",
    "ax.set_aspect(1.0/ax.get_data_ratio(), adjustable='box')\n",
    "plt.title(r'$K(t,x)- \\hat{K}(t,x)$' ' of Proposed PIML Model', fontsize=45)\n",
    "plt.xlabel('time step (1 time step = 15 minutes)', fontsize=35)\n",
    "plt.ylabel('Remaining Trip Distance (x 1 mile)', fontsize=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3083c323",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "ax1 = plt.gca() \n",
    "ax1.set_facecolor('white')\n",
    "ax1.plot(K_array.flatten(), linewidth = '1', color = \"royalblue\", label='Loss (left)')\n",
    "plt.title(\"Observed data - Estimated data(w/ Physics)\", fontsize=30, fontweight=\"bold\")\n",
    "ax1.set_xlabel(\"Data\", fontsize=24, fontname = 'Arial', fontweight=\"bold\")\n",
    "ax1.set_ylabel(\"Observed data - Estimated data\", fontsize=24, fontname = 'Arial', fontweight=\"bold\")\n",
    "\n",
    "fig.set_size_inches(16, 9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
